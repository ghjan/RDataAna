#线性判别法
G=c(1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2)
x1=c(-1.9,-6.9,5.2,5.0,7.3,6.8,0.9,-12.5,1.5,3.8,0.2,-0.1,0.4,2.7,2.1,-4.6,-1.7,-2.6,2.6,-2.8)
x2=c(3.2,0.4,2.0,2.5,0.0,12.7,-5.4,-2.5,1.3,6.8,6.2,7.5,14.6,8.3,0.8,4.3,10.9,13.1,12.8,10.0)
a=data.frame(G,x1,x2)
plot(x1,x2)
text(x1,x2,G,adj=-0.5)

library(MASS)
ld=lda(G~x1+x2)

z=predict(ld)
newG=z$class

y=cbind(G,z$x,newG)

#距离判别法
#马氏距离
#mahalanobis()
#example0801.R
classX1<-data.frame(
    x1=c(6.60,  6.60,  6.10,  6.10,  8.40,  7.2,   8.40,  7.50,  
         7.50,  8.30,  7.80,  7.80),
    x2=c(39.00, 39.00, 47.00, 47.00, 32.00,  6.0, 113.00, 52.00,
         52.00,113.00,172.00,172.00),
    x3=c(1.00,  1.00,  1.00,  1.00,  2.00,  1.0,   3.50,  1.00,
         3.50,  0.00,  1.00,  1.50),
    x4=c(6.00,  6.00,  6.00,  6.00,  7.50,  7.0,   6.00,  6.00,
         7.50,  7.50,  3.50,  3.00),
    x5=c(6.00, 12.00,  6.00, 12.00, 19.00, 28.0,  18.00, 12.00,
         6.00, 35.00, 14.00, 15.00),
    x6=c(0.12,  0.12,  0.08,  0.08,  0.35,  0.3,   0.15,  0.16,
         0.16,  0.12,  0.21,  0.21),
    x7=c(20.00, 20.00, 12.00, 12.00, 75.00, 30.0,  75.00, 40.00,
         40.00,180.00, 45.00, 45.00)
)
classX2<-data.frame(
    x1=c(8.40,  8.40,  8.40,  6.3, 7.00,  7.00,  7.00,  8.30,
         8.30,   7.2,   7.2,  7.2, 5.50,  8.40,  8.40,  7.50,
         7.50,  8.30,  8.30, 8.30, 8.30,  7.80,  7.80),
    x2=c(32.0 ,32.00, 32.00, 11.0, 8.00,  8.00,  8.00, 161.00,
        161.0,   6.0,   6.0,  6.0, 6.00,113.00,113.00,  52.00,
        52.00, 97.00, 97.00,89.00,56.00,172.00,283.00),
    x3=c(1.00,  2.00,  2.50,  4.5, 4.50,  6.00,  1.50,  1.50,
         0.50,   3.5,   1.0,  1.0, 2.50,  3.50,  3.50,  1.00,
         1.00,  0.00,  2.50, 0.00, 1.50,  1.00,  1.00),
    x4=c(5.00,  9.00,  4.00,  7.5, 4.50,  7.50,  6.00,  4.00,
         2.50,   4.0,   3.0,  6.0, 3.00,  4.50,  4.50,  6.00,
         7.50,  6.00,  6.00, 6.00, 6.00,  3.50,  4.50),
    x5=c(4.00, 10.00, 10.00,  3.0, 9.00,  4.00,  1.00,  4.00,
         1.00,  12.0,   3.0,  5.0, 7.00,  6.00,  8.00,  6.00,
         8.00,  5.00,  5.00,10.00,13.00,  6.00,  6.00),
    x6=c(0.35,  0.35,  0.35,  0.2, 0.25,  0.25,  0.25,  0.08,
         0.08,  0.30,   0.3,  0.3, 0.18,  0.15,  0.15,  0.16,
         0.16,  0.15,  0.15, 0.16, 0.25,  0.21,  0.18),
    x7=c(75.00, 75.00, 75.00,  15.0, 30.00, 30.00, 30.00, 70.00,
         70.00,  30.0,  30.0,  30.0, 18.00, 75.00, 75.00, 40.00,
         40.00,180.00,180.00,180.00,180.00, 45.00, 45.00)
)
source("discriminiant.distance.R")
discriminiant.distance(classX1, classX2, var.equal=TRUE)
discriminiant.distance(classX1, classX2)


#贝叶斯分类器

# 薛毅 exam0803.R
TrnX1<-matrix(
   c(24.8, 24.1, 26.6, 23.5, 25.5, 27.4, 
     -2.0, -2.4, -3.0, -1.9, -2.1, -3.1),
   ncol=2)
TrnX2<-matrix(
   c(22.1, 21.6, 22.0, 22.8, 22.7, 21.5, 22.1, 21.4, 
     -0.7, -1.4, -0.8, -1.6, -1.5, -1.0, -1.2, -1.3),
   ncol=2)
setwd('R:/dataguru/RDataAna/week12')

source("discriminiant.bayes.R")

#### 样本协方差相同
discriminiant.bayes(TrnX1, TrnX2, rate=8/6, var.equal=TRUE)
#### 样本协方差不同
discriminiant.bayes(TrnX1, TrnX2, rate=8/6)

#Fisher判别
#Fisher判别是按类内方差尽量小，类间方差尽量大的准则来求判别函数的，在这里讨论两个总体的判别方法
classX1<-data.frame(
    x1=c(6.60,  6.60,  6.10,  6.10,  8.40,  7.2,   8.40,  7.50,  
         7.50,  8.30,  7.80,  7.80),
    x2=c(39.00, 39.00, 47.00, 47.00, 32.00,  6.0, 113.00, 52.00,
         52.00,113.00,172.00,172.00),
    x3=c(1.00,  1.00,  1.00,  1.00,  2.00,  1.0,   3.50,  1.00,
         3.50,  0.00,  1.00,  1.50),
    x4=c(6.00,  6.00,  6.00,  6.00,  7.50,  7.0,   6.00,  6.00,
         7.50,  7.50,  3.50,  3.00),
    x5=c(6.00, 12.00,  6.00, 12.00, 19.00, 28.0,  18.00, 12.00,
         6.00, 35.00, 14.00, 15.00),
    x6=c(0.12,  0.12,  0.08,  0.08,  0.35,  0.3,   0.15,  0.16,
         0.16,  0.12,  0.21,  0.21),
    x7=c(20.00, 20.00, 12.00, 12.00, 75.00, 30.0,  75.00, 40.00,
         40.00,180.00, 45.00, 45.00)
)
classX2<-data.frame(
    x1=c(8.40,  8.40,  8.40,  6.3, 7.00,  7.00,  7.00,  8.30,
         8.30,   7.2,   7.2,  7.2, 5.50,  8.40,  8.40,  7.50,
         7.50,  8.30,  8.30, 8.30, 8.30,  7.80,  7.80),
    x2=c(32.0 ,32.00, 32.00, 11.0, 8.00,  8.00,  8.00, 161.00,
        161.0,   6.0,   6.0,  6.0, 6.00,113.00,113.00,  52.00,
        52.00, 97.00, 97.00,89.00,56.00,172.00,283.00),
    x3=c(1.00,  2.00,  2.50,  4.5, 4.50,  6.00,  1.50,  1.50,
         0.50,   3.5,   1.0,  1.0, 2.50,  3.50,  3.50,  1.00,
         1.00,  0.00,  2.50, 0.00, 1.50,  1.00,  1.00),
    x4=c(5.00,  9.00,  4.00,  7.5, 4.50,  7.50,  6.00,  4.00,
         2.50,   4.0,   3.0,  6.0, 3.00,  4.50,  4.50,  6.00,
         7.50,  6.00,  6.00, 6.00, 6.00,  3.50,  4.50),
    x5=c(4.00, 10.00, 10.00,  3.0, 9.00,  4.00,  1.00,  4.00,
         1.00,  12.0,   3.0,  5.0, 7.00,  6.00,  8.00,  6.00,
         8.00,  5.00,  5.00,10.00,13.00,  6.00,  6.00),
    x6=c(0.35,  0.35,  0.35,  0.2, 0.25,  0.25,  0.25,  0.08,
         0.08,  0.30,   0.3,  0.3, 0.18,  0.15,  0.15,  0.16,
         0.16,  0.15,  0.15, 0.16, 0.25,  0.21,  0.18),
    x7=c(75.00, 75.00, 75.00,  15.0, 30.00, 30.00, 30.00, 70.00,
         70.00,  30.0,  30.0,  30.0, 18.00, 75.00, 75.00, 40.00,
         40.00,180.00,180.00,180.00,180.00, 45.00, 45.00)
)
source("discriminiant.fisher.R")
##有两个错误
discriminiant.fisher(classX1, classX2)

#多分类下的贝叶斯
# exam0804.R
X<-iris[,1:4]
G<-gl(3,50) #Generate Factor Levels
setwd('R:/dataguru/RDataAna/week12')
source("distinguish.bayes.R")
distinguish.bayes(X,G)

#多分类下的距离判别法
#薛毅 exam0802.R
X<-iris[,1:4]
G<-gl(3,50)
source("distinguish.distance.R")
##错误:71,73,84
distinguish.distance(X,G)

#------------------------开始几种数据挖掘的算法
#http://f.dataguru.cn/thread-584303-1-1.html
#韩家炜的书
#最近邻算法knn
#
#

#决策树decision tree
#输入：学习集
#输出：分类规则（决策树）
#ID3算法实现决策树
#以鸢尾花数据集为例
library(rpart)

iris.rp = rpart(Species~., data=iris, method="class")
plot(iris.rp, uniform=T, branch=0, margin=0.1, 
    main=" Classification Tree\nIris Species by Petal and Sepal Length")
text(iris.rp, use.n=T, fancy=T, col="blue")

##数据分析之美：决策树R语言实现
##http://blog.csdn.net/yangzhongblog/article/details/47151837
##数据集：ISLR包中的Carseats数据集
##见example_decision_tree.R


#decision_tree.R里面实现了gen_decision_tree()，根据训练集生成决策树
---------------决策树总结-------------------
1、R中有实现决策树算法的包rpart，和画出决策树的包rpart.plot，本例自己实现决策树算法是为了更好的理解。
2、由于决策树只能处理离散属性，因此连续属性应首先进行离散化。
3、决策树易于理解，对业务的解释性较强。
4、ID3算法容易引起过拟合，需考虑树的剪枝。

#r语言做决策树模型（少废话版本
#http://blog.csdn.net/li603060971/article/details/57080802
#画决策树第2种方法，画出来的树稍微好看些
library(rpart.plot)
rpart.plot(tree.both,branch=1,shadow.col="gray",
    box.col="green",border.col="blue",split.col="red",split.cex=1.2,main="决策树")
